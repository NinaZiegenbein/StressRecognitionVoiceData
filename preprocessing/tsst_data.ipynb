{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preprocessing of TSST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from IPython.display import Video, Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Colletion of Videos and creating Audios\n",
    "First I collect all TSST video files (including the ones from the second camera, but they are currently not used) and convert them to mp3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all TSST video files\n",
    "video_files_tsst = glob.glob(\"/data/dst_tsst_22_bi_multi_nt_lab/raw/mainstudy/**/*.MOV\",recursive=True)\n",
    "video_files_tsst2 = glob.glob(\"/data/dst_tsst_22_bi_multi_nt_lab/raw/mainstudy/**/*.MP4\",recursive=True)\n",
    "\n",
    "# filter out all _2 videos (video camera splits after 12:30min into two files, speech task always in first video)\n",
    "video_files_tsst = [path for path in video_files_tsst if not re.search(\"_2.MOV$\", path, re.IGNORECASE)]\n",
    "video_files_tsst2 = [path for path in video_files_tsst2 if not re.search(\"_2.MOV$\", path, re.IGNORECASE)]\n",
    "\n",
    "# wrong filename (corrected on vmc, but not synched yet so here done by hand -> can be deleted afterwards)\n",
    "video_files_tsst = [path for path in video_files_tsst if not re.search(\"_1b.MOV$\", path, re.IGNORECASE)]\n",
    "\n",
    "print(\"I found\", len(video_files_tsst), \"TSST videos\")\n",
    "print(\"I found\", len(video_files_tsst2), \"secondary TSST videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all .MOV files into wav files - takes a while\n",
    "audio_files_tsst = []\n",
    "for input_file in video_files_tsst:\n",
    "    output_file = \"/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/\" + input_file.rsplit('/', 1)[1][:-3] + \"wav\"\n",
    "    print(input_file, output_file)\n",
    "    # skip creating .wav file if it already exists\n",
    "    if not os.path.exists(output_file):\n",
    "        clip = VideoFileClip(input_file)\n",
    "        clip.audio.write_audiofile(output_file) #, codec='pcm_s16le'\n",
    "    audio_files_tsst.append(output_file)\n",
    "    AudioFileClip(output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Segmentation of Audio File to only include speech task\n",
    "Next the start of the speech task is manually checked and saved in dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start-times for segmentation in seconds after first camera start (manual inspection)\n",
    "segment_starts = {'CZ513556_tsst_video': 35, 'CS181122_tsst_video': 30, 'PD513556_tsst_video_': 30, 'JK261022_tsst_video': 49, 'AZ573556_tsst_video': 0, 'JB011222_tsst_video': 23, 'DQ563556_tsst_video': 28, 'DK011122_tsst_video': 28, 'SB041122_tsst_video': 27, 'DC553556_tsst_video': 28, 'AS050123_tsst_video': 30, 'ML031122_tsst_video': 32, 'MK230123_tsst_video_1': 32, 'MX463556_tsst_video': 30, 'MG130123_tsst_video': 24, 'KO433656_tsst_video': 29, 'SB021122_tsst_video': 32, 'SE141122_tsst_video': 30, 'EC250123_tsst_video_1': 28, 'KK483556_tsst_video_1a': 33, 'SS291122_tsst_movie': 33, 'MS021222_tsst_video': 28, 'KT463556_tsst_video': -1, 'JB190123_tsst_video': 30, 'OQ503556_tsst_video': 28, 'NE563556_tsst_video': 28, \"TB493656_tsst_video\":29, \"NI433856_tsst_video\":30, \"JM463656_tsst_video\":28, \"BS323856_tsst_video\":27, \"SB443756_tsst_video\":30, \"KH553656_tsst_video\":28, \"FC483856_tsst_video\":28, \"TF483656_tsst_video\":28, \"JH373756_tsst_video\":26, \"OM423756_tsst_video\":29, \"KK483556_tsst_video_1\": 33, \"TZ493156_tsst_video\":33, \"NM443056_tsst_video\":28, \"WV453056_tsst_video\":31, \"BU563856_tsst_video\":31, \"BI343156_tsst_video\":42, \"ML373056_tsst_video\":29, \"BC493156_tsst_video\": 38, \"UH473956_tsst_video\": 30, \"BH373056_tsst_video\": 40, \"TO523956_tsst_video\":30}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for new audios, which are not transcribed with start time in the dictionary above\n",
    "to_check = []\n",
    "#print(sorted(segment_starts.keys()))\n",
    "for video_path in video_files_tsst:\n",
    "    #token = video_path[48:56]\n",
    "    token =  video_path.rsplit('/', 1)[1][:-4]\n",
    "    if token not in segment_starts.keys():\n",
    "        audio_path = \"/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/\" + token + \".wav\"\n",
    "        to_check.append(audio_path)\n",
    "print(\"to_check\",to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Snippet to listen and manually transcribe start time for new audios\n",
    "sample_audio = to_check[9]\n",
    "print(sample_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import AudioFileClip\n",
    "\n",
    "audio_clip = AudioFileClip(sample_audio)\n",
    "\n",
    "# Display the video clip\n",
    "audio_clip.ipython_display(width=400, maxduration=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(sample_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_audio(audio_file, start):\n",
    "    \"\"\"\n",
    "    This function segments an audio file from a given start time to 5 minutes later, to capture only the speech task as an audio file.\n",
    "    @param audio_file: path to audio file\n",
    "    @param start: start of speech task in seconds\n",
    "    @return: path to segmented audio file\n",
    "    \"\"\"\n",
    "    audio = AudioFileClip(audio_file)\n",
    "    segment_length = 300 # 5minutes\n",
    "    end = start + segment_length\n",
    "    new_path = \"/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/\" + audio_file.rsplit('/', 1)[1][:-4] + \"_segment.wav\"\n",
    "    if start == -1:\n",
    "        return None\n",
    "    if not os.path.exists(new_path):\n",
    "        segment = audio.subclip(start, end)\n",
    "        segment.write_audiofile(new_path, fps=16000) # downsample to 16Hz\n",
    "    return new_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Creating and Filtering DataFrame\n",
    "\n",
    "I first create a dataframe with token, video_path(s), audio_path, segmented audio path and segment start in seconds. I then merge it with VAS self-assessed stress information from the participant.csv and calculate the delat before and after the stress test, as well filter out anyone that does not speak German as a first language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data = {}\n",
    "for vfile in (video_files_tsst + video_files_tsst2):\n",
    "    token = vfile.split(\"/\")[-1][:8]\n",
    "    if \"cam\" in vfile:\n",
    "        name = \"TSST2\"\n",
    "    else:\n",
    "        name = \"TSST\"\n",
    "    if token not in video_data:\n",
    "        video_data[token] = {\"TSST\": None, \"TSST2\": None}\n",
    "    video_data[token][name] = vfile\n",
    "\n",
    "audio_data = {}\n",
    "for audio_name, start_num in segment_starts.items():\n",
    "    audio_file = \"/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/\" + audio_name + \".wav\"\n",
    "    token = audio_name[:8]\n",
    "    audio_segment = segment_audio(audio_file, start_num)\n",
    "    if token not in audio_data:\n",
    "        audio_data[token] = {\"audio_file\": None, \"audio_segment\": None, \"segment_start\": None}\n",
    "    audio_data[token][\"audio_file\"] = audio_file\n",
    "    audio_data[token][\"audio_segment\"] = audio_segment\n",
    "    audio_data[token][\"segment_start\"] = start_num\n",
    "\n",
    "data = []\n",
    "for token, values in video_data.items():\n",
    "    audio_file = audio_data[token][\"audio_file\"] if token in audio_data else None\n",
    "    audio_segment = audio_data[token][\"audio_segment\"] if token in audio_data else None\n",
    "    segment_start = audio_data[token][\"segment_start\"] if token in audio_data else None\n",
    "    data.append([token, values[\"TSST\"], values[\"TSST2\"], audio_file, audio_segment, segment_start])\n",
    "\n",
    "tsst_data = pd.DataFrame(data, columns=[\"token\", \"TSST_video\", \"TSST2_video\", \"TSST_audio\", \"TSST_audio_segment\", \"segment_start\"])\n",
    "display(tsst_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_duration(video_path):\n",
    "    \"\"\"\n",
    "    Function to get the duration of a video\n",
    "    @param video_path: path to video\n",
    "    @return: duration of clip in seconds\n",
    "    \"\"\"\n",
    "    clip = VideoFileClip(video_path)\n",
    "    return clip.duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all participant information\n",
    "participants = pd.read_csv(\"/data/dst_tsst_22_bi_multi_nt_lab/processed/participant.csv\")\n",
    "display(participants[[\"token\", \"tsst_vas_stress_T1\", \"tsst_vas_stress_T2\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo_token = {0:'ML031122', 1:'MK230123', 2:'JB011222', 3:'DK011122', 4:'DC553556', 5:'WV453056', 6:'JB190123', 7:'SS291122', 8:'AS050123', 9:'TO523956', 10:'JK261022'}\n",
    "\n",
    "test_files_280 = ['NE563556', 'JM463656', 'TF483656']\n",
    "\n",
    "loo = ['/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/ML031122_tsst_video_segment.wav'\n",
    ",'/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/MK230123_tsst_video_1_segment.wav'\n",
    ",'/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/JB011222_tsst_video_segment.wav'\n",
    ",'/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/DK011122_tsst_video_segment.wav'\n",
    ",'/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/DC553556_tsst_video_segment.wav'\n",
    ",'/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/WV453056_tsst_video_segment.wav'\n",
    ",'/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/JB190123_tsst_video_segment.wav'\n",
    ",'/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/SS291122_tsst_movie_segment.wav'\n",
    ",'/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/AS050123_tsst_video_segment.wav'\n",
    ",'/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/TO523956_tsst_video_segment.wav'\n",
    ",'/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/JK261022_tsst_video_segment.wav']\n",
    "sample_audio = loo[0]\n",
    "audio_clip = AudioFileClip(sample_audio)\n",
    "Audio(sample_audio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import AudioFileClip\n",
    "# Display the video clip\n",
    "audio_clip.ipython_display(width=400, maxduration=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete non-native German speakers from dataframe\n",
    "print(\"before mothertongue\", len(tsst_data))\n",
    "tokens_to_drop = participants.loc[participants['mothertongue'] != 'Deutsch', 'token'].tolist()\n",
    "tsst_data = tsst_data[~tsst_data['token'].isin(tokens_to_drop)]\n",
    "print(\"after mothertongue\", len(tsst_data))\n",
    "\n",
    "# delete all length less than 10minutes (original video) (and delete all NaN videos)\n",
    "tsst_data = tsst_data[tsst_data['TSST_video'].apply(lambda x: get_video_duration(x) >= 300 if pd.notnull(x) else False)]\n",
    "print(\"after > 10min original video\", len(tsst_data))\n",
    "\n",
    "# add vas_stress data and calculate delta\n",
    "tsst_data = pd.merge(tsst_data, participants[['token','tsst_vas_stress_T1','tsst_vas_stress_T2', 'panel_passivevas_speechstress0', 'panel_activevas_speechstress0']], on=\"token\", how=\"inner\")\n",
    "tsst_data['stress_delta'] = tsst_data['tsst_vas_stress_T2'] - tsst_data['tsst_vas_stress_T1']\n",
    "tsst_data = tsst_data.dropna(subset=[\"panel_passivevas_speechstress0\", \"panel_activevas_speechstress0\"])\n",
    "tsst_data['panel_stress_speech_average'] = (tsst_data['panel_passivevas_speechstress0'] + tsst_data['panel_activevas_speechstress0']) / 2\n",
    "\n",
    "print(\"after self-assessed stress merge\")\n",
    "\n",
    "# add panel vas_stress (absolut not relative, since only one measurement)\n",
    "#participants[\"\"]\n",
    "\n",
    "\n",
    "\n",
    "display(tsst_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    panel_stress = tsst_data.loc[tsst_data['token'] == test_files_280[i], 'panel_stress_speech_average'].values[0]\n",
    "    print(i, panel_stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsst_data.to_csv(\"/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/tsst_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY CODE UNTIL VM-PROBLEMS SORTED\n",
    "panel = pd.read_csv(\"/homes/nziegenbein/panel_survey.csv\")\n",
    "panel_stress_speech = panel[[\"token\", \"panel_passivevas_speechstress0\", \"panel_activevas_speechstress0\"]].dropna()\n",
    "panel_stress_speech['panel_stress_speech_average'] = (panel_stress_speech['panel_passivevas_speechstress0'] + panel_stress_speech['panel_activevas_speechstress0']) / 2\n",
    "display(panel_stress_speech)\n",
    "\n",
    "# delete non-native German speakers from dataframe\n",
    "tokens_to_drop = participants.loc[participants['mothertongue'] != 'Deutsch', 'token'].tolist()\n",
    "tsst_data = tsst_data[~tsst_data['token'].isin(tokens_to_drop)]\n",
    "\n",
    "# delete all length less than 10minutes (original video) (and delete all NaN videos)\n",
    "tsst_data = tsst_data[tsst_data['TSST_video'].apply(lambda x: get_video_duration(x) >= 600 if pd.notnull(x) else False)]\n",
    "\n",
    "# add vas_stress data and calculate delta\n",
    "tsst_data = pd.merge(tsst_data, panel_stress_speech, on=\"token\", how=\"inner\")\n",
    "display(tsst_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Trying Butterworth (lowpass) Filter to reduce noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(tsst_data[\"TSST_audio_segment\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Load the audio file\n",
    "fs, data = wavfile.read(tsst_data[\"TSST_audio_segment\"][0])\n",
    "\n",
    "# Define the cutoff frequency and filter order\n",
    "cutoff_freq = 1200  # Adjust this value based on your needs\n",
    "order = 6\n",
    "\n",
    "# Apply the low-pass filter\n",
    "filtered_data = butter_lowpass_filter(data, cutoff_freq, fs, order)\n",
    "\n",
    "temp_path = \"/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/temp_file.wav\"\n",
    "wavfile.write(temp_path, fs, (filtered_data * 32767).astype(np.int16))\n",
    "\n",
    "Audio(temp_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Saving DataFrame as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Visualizing panel assessed stress distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = pd.read_csv(\"/data/dst_tsst_22_bi_multi_nt_lab/raw/dstvtsst_limesurvey_panel_28-02-23.csv\")\n",
    "display(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "panel['VAS0Stress0Passive[SQ001]'].hist(bins=[0,10,20,30,40,50,60,70,80,90,100])\n",
    "plt.title(\"Distribution of panel-assessed stress (passive)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel['VAS0Stress0Active[SQ001]'].hist(bins=[0,10,20,30,40,50,60,70,80,90,100])\n",
    "plt.title(\"Distribution of panel-assessed stress (active)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel['panel_stress'] = (panel['VAS0Stress0Active[SQ001]'] + panel['VAS0Stress0Passive[SQ001]']) / 2\n",
    "panel['panel_stress'].hist(bins=[0,10,20,30,40,50,60,70,80,90,100])\n",
    "plt.title(\"Distribution of panel-assessed stress (averaged)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel['panel_stress_difference'] = abs(panel['VAS0Stress0Active[SQ001]'] - panel['VAS0Stress0Passive[SQ001]'])\n",
    "panel['panel_stress_difference'].hist(bins=[0,10,20,30,40,50,60,70,80,90,100])\n",
    "plt.title(\"Distribution of difference in panel-assessed stress between panel members\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Visualization of cortisol and alpha-amylase stress distribution\n",
    "\n",
    "Value inferred as peak-reactivity according to Miller, which is the difference between the peak (highest value) and the baseline (measurement T1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(participants)\n",
    "participants[\"tsst_amylase_mean_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\"T1\", \"T2\", \"T3\", \"T4\", \"T5\"]\n",
    "for index, row in  participants.filter(like=\"tsst_cortisol_mean\").iterrows():\n",
    "    y = row\n",
    "    plt.plot(x,y,  marker='o', linestyle='--')\n",
    "plt.xlabel('Time Points')\n",
    "plt.ylabel('Cortisol Mean')\n",
    "plt.title('Trajectories of Cortisol Mean')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to handle commas as dots while converting to float\n",
    "def custom_to_float(value):\n",
    "    if isinstance(value, str) and ',' in value:\n",
    "        return float(value.replace(',', '.'))\n",
    "    return float(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum value among \"T2\" to \"T5\" columns for each row\n",
    "participants = participants[['tsst_cortisol_mean_1','tsst_cortisol_mean_2', 'tsst_cortisol_mean_3', 'tsst_cortisol_mean_4', 'tsst_cortisol_mean_5']].applymap(custom_to_float)\n",
    "participants['tsst_cortisol_peak'] = participants[['tsst_cortisol_mean_2', 'tsst_cortisol_mean_3', 'tsst_cortisol_mean_4', 'tsst_cortisol_mean_5']].max(axis=1)\n",
    "# Calculate the difference between \"baseline\" and \"peak\"\n",
    "participants['tsst_cortisol_peak_difference'] = participants['tsst_cortisol_peak'] - participants['tsst_cortisol_mean_1']\n",
    "participants['tsst_cortisol_peak_difference'].hist(bins=[-5,0,5,10,15,20,25,30,35])\n",
    "plt.title(\"Distribution of cortisol peak-reactivity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(participants.filter(like=\"tsst_amylase_mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = participants[['tsst_amylase_mean_1','tsst_amylase_mean_2', 'tsst_amylase_mean_3', 'tsst_amylase_mean_4', 'tsst_amylase_mean_5']].applymap(custom_to_float)\n",
    "x = [\"T1\", \"T2\", \"T3\", \"T4\", \"T5\"]\n",
    "for index, row in participants.filter(like=\"tsst_amylase_mean\").iterrows():\n",
    "\ty = row\n",
    "\tplt.plot(x, y, marker='o', linestyle='--')\n",
    "plt.xlabel('Time Points')\n",
    "plt.ylabel('Amylase Mean')\n",
    "plt.title('Trajectories of Amylase Mean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants['tsst_amylase_peak'] = participants[['tsst_amylase_mean_2', 'tsst_amylase_mean_3', 'tsst_amylase_mean_4', 'tsst_amylase_mean_5']].max(axis=1)\n",
    "# Calculate the difference between \"baseline\" and \"peak\"\n",
    "participants['tsst_amylase_peak_difference'] = participants['tsst_amylase_peak'] - participants['tsst_amylase_mean_1']\n",
    "participants['tsst_amylase_peak_difference'].hist(bins=[-100,-50,0,50,100,150,200,250,300,350,400,450,500,550, 600,650,700])\n",
    "plt.title(\"Distribution of amylase peak-reactivity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
