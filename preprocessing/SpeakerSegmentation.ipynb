{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pydub\n",
    "import re\n",
    "import os\n",
    "from IPython.display import Video, Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Speaker Segmentation\n",
    "\n",
    "Here I will try to identify panel speech, so it can be cut out from the audio segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding good example Audios\n",
    "\n",
    "# Import tsst-data dataframe\n",
    "tsst_data = pd.read_csv(\"/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/tsst_data.csv\", index_col=0)\n",
    "sample_tokens = [\"SS291122\", \"ML031122\", \"NE563556\", \"JB011222\"]\n",
    "panel_intervals = {\"SS291122\": [], \"ML031122\": [(133, 138), (196, 199)], \"NE563556\": [],\n",
    "\t\t\t\t\t   \"JB011222\": [(149, 151), (173, 176), (213, 215), (241, 244), (275, 278)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### \"Transcripts\" of Sample Segments\n",
    "\n",
    "**SS291122** male participant\n",
    "\n",
    "| Time |  |\n",
    "|-----------| --- |\n",
    "| 4:15-4:25 | silence |\n",
    "\n",
    "\n",
    "**ML031122** male participant\n",
    "\n",
    "| Time      | <!-- -->    |\n",
    "|-----------|-------------|\n",
    "| 1:47-2:13 | silence |\n",
    "| 2:13-2:18 | panel |\n",
    "| 2:20-2:28 | silence |\n",
    "| 2:39-3:16 | silence |\n",
    "| 3:16-3:19 | panel |\n",
    "| 4:08-4:14 | silence |\n",
    "| 4:43-4:49 | silence |\n",
    "\n",
    "**NE563556** female participant\n",
    "\n",
    "| Time      |         |\n",
    "|-----------|---------|\n",
    "| 4:39-5:00 | silence |\n",
    "\n",
    "**JB011222** female participant\n",
    "\n",
    "| Time      |         |\n",
    "|-----------|---------|\n",
    "| 1:06-1:10 | silence |\n",
    "| 2:07-2:29 | silence |\n",
    "| 2:29-2:31 | panel |\n",
    "| 2:36-2:53 | silence |\n",
    "| 2:53-2:56 | panel |\n",
    "| 3:06-3:10 | silence |\n",
    "| 3:15-3:33 | silence |\n",
    "| 3:33-3:35 | panel |\n",
    "| 3:35-4:01 | silence |\n",
    "| 4:01-4:04 | panel |\n",
    "| 4:13-4:35 | silence |\n",
    "| 4:35-4:38 | panel |\n",
    "| 4:38-4:52 | silence |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tsst_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to go through audio segments\n",
    "participant = 19\n",
    "sample_audio = tsst_data[\"TSST_audio_segment\"][participant]\n",
    "sample_token = tsst_data[\"token\"][participant]\n",
    "print(sample_token)\n",
    "Audio(sample_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### InaSpeechSegmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inaSpeechSegmenter import Segmenter, seg2csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter(vad_engine='smn', detect_gender=True)\n",
    "#segmenter = Segmenter(vad_engine='smn', detect_gender=False)\n",
    "\n",
    "for sample_token in sample_tokens:\n",
    "\tsample_audio = tsst_data.loc[tsst_data['token'] == sample_token, 'TSST_audio_segment'].values[0]\n",
    "\tsegmentation = segmenter(sample_audio)\n",
    "\tprint(sample_token)\n",
    "\tprint(segmentation, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**detect_gender=True**\n",
    "Works well for male participants where the panel is correctly classified as female for both example audios with correct start/end times of the panel speaking. It does not work for females at all, there are many misclassifications (eg. 11times for NE563556) as male when there is silence or when the female participant is speaking. Even the start/end times are incorrect.\n",
    "\n",
    "Without detect_gender there is no distinction between speakers, other vad-engine is sm, which only distinguishes between speech and music and not noise. No further settings to tweak. **Not an option**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### AudioSegmentation from pyAudioAnalyis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pyAudioAnalysis import audioSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_token in sample_tokens:\n",
    "\tsample_audio = tsst_data.loc[tsst_data['token'] == sample_token, 'TSST_audio_segment'].values[0]\n",
    "\n",
    "\t\"\"\"\n",
    "\t# Convert the mp3 file to wav-format\n",
    "\taudio = AudioSegment.from_file(sample_audio, format=\"wav\")\n",
    "\twav_path = sample_audio[:-3] + \"wav\"\n",
    "\taudio.export(wav_path, format=\"wav\")\n",
    "\t\"\"\"\n",
    "\tprint(sample_token)\n",
    "\taudioSegmentation.speaker_diarization(sample_audio, n_speakers=2, lda_dim=1, plot_res=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Parameters:\n",
    "- n-speakers: number of speakers (clusters)\n",
    "- lda-dim: LDA(Linear Discriminant Analysis) dimension (0 for no LDA), uses GaussianHMM instead\n",
    "- plot_res: polt results yes/no\n",
    "\n",
    "**No LDA (lda-dim=0)**\n",
    "Does not show good results, way to many speaker switches.\n",
    "\n",
    "**lda-dim=1**\n",
    "If lda-dim=C-1 then this would be 1 in oour case, since we have two speakers. This does not make the results better at all.\n",
    "\n",
    "**lda-dim=2**\n",
    "Less speaker switches, but not correct ones.\n",
    "\n",
    "**Not and option**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Diarization by pydiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydiar.models import BinaryKeyDiarizationModel, Segment\n",
    "from pydiar.util.misc import optimize_segments\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 32000\n",
    "\n",
    "for sample_token in sample_tokens:\n",
    "\tsample_audio = tsst_data.loc[tsst_data['token'] == sample_token, 'TSST_audio_segment'].values[0]\n",
    "\taudio = AudioSegment.from_file(sample_audio, format=\"wav\")\n",
    "\taudio = audio.set_frame_rate(sample_rate)\n",
    "\taudio = audio.set_channels(1)\n",
    "\tdiarization_model = BinaryKeyDiarizationModel()\n",
    "\tsegments = diarization_model.diarize(sample_rate, np.array(audio.get_array_of_samples()))\n",
    "\toptimized_segments = optimize_segments(segments, skip_short_limit=2)\n",
    "\tprint(sample_token)\n",
    "\tprint(optimized_segments)\n",
    "\tprint(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To my surprise generates the BinaryDiarizationModel four speaker IDs for SS291122. The diarization for ML031122 is completely correct (id=2.0 is the panel). For NE563556 is completely correct as well, it only predicts one speaker for (almost 5 minutes). Unfortunately for JB011222 it also predicts only one speaker and misses the 5 times the panel is speaking.\n",
    "\n",
    "It works great for two scenarios (male with panel, female without panel) and terrible for the other two. Next step is to see if parameters can be tweaked. There are no parameters for BinaryKeyDiarizationModel, diarize (sample_rate and audio). There are some parameters for optimize segments:\n",
    "- keep_gaps=False -> if silence should be cut out this should be true (we do not want this)\n",
    "- skip_short_limit=0.5 -> if there are segments made by the model with less than x (0.5seconds) they should be cut out. The panel speaks on average for about 3 seconds, so I set this to 2 seconds.\n",
    "\n",
    "But since the segmentation is already done at that point, this tweaked parameter just removes very small segmentations, but does not make the model perform better for the two participants that it does not work well for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### pyannote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. visit hf.co/pyannote/speaker-diarization and hf.co/pyannote/segmentation and accept user conditions (only if requested)\n",
    "# 2. visit hf.co/settings/tokens to create an access token (only if you had to go through 1.)\n",
    "# 3. instantiate pretrained speaker diarization pipeline\n",
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\",\n",
    "                                    use_auth_token=\"ACCESS_TOKEN_GOES_HERE\")\n",
    "\n",
    "# 4. apply pretrained pipeline\n",
    "diarization = pipeline(\"audio.wav\")\n",
    "\n",
    "# 5. print the result\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n",
    "# start=0.2s stop=1.5s speaker_0\n",
    "# start=1.8s stop=3.9s speaker_1\n",
    "# start=4.2s stop=5.7s speaker_0\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "pipeline = torch.hub.load('pyannote/pyannote-audio', 'dia_ami')\n",
    "# speech activity detection model trained on AMI training set\n",
    "sad = torch.hub.load('pyannote/pyannote-audio', 'sad_ami')\n",
    "# speaker change detection model trained on AMI training set\n",
    "scd = torch.hub.load('pyannote/pyannote-audio', 'scd_ami')\n",
    "# overlapped speech detection model trained on AMI training set\n",
    "ovl = torch.hub.load('pyannote/pyannote-audio', 'ovl_ami')\n",
    "# speaker embedding model trained on AMI training set\n",
    "emb = torch.hub.load('pyannote/pyannote-audio', 'emb_ami')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "\n",
    "#pipeline = pipelines.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=\"ACCESS_TOKEN_GOES_HERE\")\n",
    "\n",
    "# 4. apply pretrained pipeline\n",
    "diarization = pipeline(sample_audio)\n",
    "\n",
    "# 5. print the result\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "\tprint(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Fundamental Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import parselmouth\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raw waveform\n",
    "for sample_token in sample_tokens:\n",
    "\tsample_audio = tsst_data.loc[tsst_data['token'] == sample_token, 'TSST_audio_segment'].values[0]\n",
    "\t# wav_path already created for sample files above\n",
    "\tsnd = parselmouth.Sound(sample_audio)\n",
    "\tplt.figure()\n",
    "\tplt.title(sample_token)\n",
    "\tplt.plot(snd.xs(), snd.values.T)\n",
    "\tplt.xlim([snd.xmin, snd.xmax])\n",
    "\tplt.xlabel(\"time [s]\")\n",
    "\tplt.ylabel(\"amplitude\")\n",
    "\tfor interval in panel_intervals[sample_token]:\n",
    "\t\tstart_time, end_time = interval\n",
    "\t\tplt.axvspan(start_time, end_time, color='red', alpha=0.3)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fundamental frequencies\n",
    "for sample_token in sample_tokens:\n",
    "\tsample_audio = tsst_data.loc[tsst_data['token'] == sample_token, 'TSST_audio_segment'].values[0]\n",
    "\t# wav_path already created for sample files above\n",
    "\tsnd = parselmouth.Sound(sample_audio[:-3]+\"wav\")\n",
    "\tchannel_left = snd.extract_left_channel()\n",
    "\tchannel_right = snd.extract_right_channel()\n",
    "\tmono = snd.convert_to_mono()\n",
    "\n",
    "\tpitch_stereo = snd.to_pitch()\n",
    "\tpitch_left = channel_left.to_pitch()\n",
    "\tpitch_right = channel_right.to_pitch()\n",
    "\tpitch_mono = mono.to_pitch()\n",
    "\n",
    "\tfig, axs = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\t# Plot the fundamental frequencies for the stereo\n",
    "\taxs[0].plot(pitch_stereo.xs(), pitch_stereo.selected_array['frequency'])\n",
    "\taxs[0].set(xlabel='Time (s)', ylabel='Fundamental Frequency (Hz)', title='Stereo '+sample_token)\n",
    "\n",
    "\t# Plot the fundamental frequencies for the left channel\n",
    "\taxs[1].plot(pitch_left.xs(), pitch_left.selected_array['frequency'])\n",
    "\taxs[1].set(xlabel='Time (s)', ylabel='Fundamental Frequency (Hz)', title='Left Channel '+sample_token)\n",
    "\n",
    "\t# Plot the fundamental frequencies for the right channel\n",
    "\taxs[2].plot(pitch_right.xs(), pitch_right.selected_array['frequency'])\n",
    "\taxs[2].set(xlabel='Time (s)', ylabel='Fundamental Frequency (Hz)', title='Right Channel '+sample_token)\n",
    "\n",
    "\t# Plot the fundamental frequencies for the converted mono\n",
    "\taxs[3].plot(pitch_mono.xs(), pitch_mono.selected_array['frequency'])\n",
    "\taxs[3].set(xlabel='Time (s)', ylabel='Fundamental Frequency (Hz)', title='Mono '+sample_token)\n",
    "\n",
    "\tfor interval in panel_intervals[sample_token]:\n",
    "\t\tfor i in range(4):\n",
    "\t\t\tstart_time, end_time = interval\n",
    "\t\t\taxs[i].axvspan(start_time, end_time, color='red', alpha=0.3)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I tried plotting the fundamental frequencies overlayed with the panel speaking for the stero file, mono file and both channels seperately. Unfortunately no none align completely with the panel. While there are spikes in frequency when the panel is speaking, there are also (higher) spikes when the panel is not speaking. Also there is always a longer break before the panel is speaking, which can be seen as lower frequencies, but that is again not reliable, since in JB011222 ther is a spike in the \"break\" and in ML031122 the \"cut-off\" for low-frequency is not consistent. Also in the other two there are spikes with longer times with low frequency, although not as distinct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Combination of Amplitude and Frequency\n",
    "\n",
    "In the amplitude the silences can be better seen, so one could pick out timepoints there after a longer break and evaluate if those timepoints sho higher frequency. This is an option, but will most likely not work perfectly for cases in which the panel gives other instructions, does not wait the full length or the participants says an \"mmh\" in the middle, but the panel does not count it as speech, so does not start the 20 seconds over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(tsst_data[\"TSST_audio_segment\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
