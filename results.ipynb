{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This file includes functions and plots for extracting loss values, predicted and true values from slurm-output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Code to extract the losses from the out-file\n",
    "\n",
    "# Initialize lists to store the losses\n",
    "train_losses = [[] for _ in range(11)]  # Assuming 5 folds\n",
    "validation_losses = [[] for _ in range(11)]\n",
    "test_losses = [[] for _ in range(11)]\n",
    "\n",
    "# Read the text file and extract losses\n",
    "with open('/homes/nziegenbein/slurm-280.out', 'r') as file:\n",
    "    for line in file:\n",
    "        if \"Fold\" in line and \"Epoch\" in line:\n",
    "            fold = int(re.search(r'Fold (\\d+)/(\\d+)', line).group(1)) - 1\n",
    "            train = float(re.search(r'Train Loss: (\\d+\\.\\d+)', line).group(1))\n",
    "            val = float(re.search(r'Validation Loss: (\\d+\\.\\d+)', line).group(1))\n",
    "            test = float(re.search(r'Test Loss:  (\\d+\\.\\d+)', line).group(1))\n",
    "\n",
    "            train_losses[fold].append(train)\n",
    "            validation_losses[fold].append(val)\n",
    "            test_losses[fold].append(test)\n",
    "print(\"train\", train_losses)\n",
    "print(\"val:\", validation_losses)\n",
    "print(\"test:\", test_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results from slurm-280.out\n",
    "# (11) fold Leave-one-out validation\n",
    "# window_size: 8 ; stride: 4\n",
    "# batch_size: 16 ; learning_rate: 0.0001\n",
    "# Using TSST v DST Data with panel stress\n",
    "\n",
    "train_losses_280 = [[0.0632, 0.0517, 0.0509, 0.0514, 0.0518, 0.0509, 0.051, 0.0509, 0.0503, 0.0504], [0.0554, 0.0484, 0.0479, 0.0479, 0.047, 0.0464, 0.0477, 0.0466, 0.0466, 0.0482], [0.0575, 0.0521, 0.0501, 0.0502, 0.05, 0.0491, 0.0494, 0.0496, 0.0495, 0.0489], [0.0545, 0.0445, 0.0436, 0.0441, 0.0436, 0.0436, 0.0433, 0.0432, 0.0435, 0.0438], [0.0584, 0.0527, 0.0504, 0.0514, 0.0501, 0.0504, 0.0503, 0.0501, 0.0507, 0.0502], [0.0526, 0.0475, 0.0458, 0.0461, 0.0474, 0.0473, 0.0459, 0.0458, 0.0455, 0.0456], [0.054, 0.0492, 0.0484, 0.0489, 0.0478, 0.0478, 0.0476, 0.0486, 0.0481, 0.0472], [0.057, 0.051, 0.049, 0.0504, 0.0492, 0.0492, 0.0482, 0.0491, 0.0489, 0.0491], [0.0572, 0.051, 0.0522, 0.0508, 0.0502, 0.05, 0.0509, 0.0512, 0.0502, 0.0507], [0.0556, 0.0511, 0.0487, 0.0489, 0.0484, 0.0481, 0.048, 0.0482, 0.0484, 0.0488]]\n",
    "validation_losses_280 = [[0.0005, 0.0053, 0.0, 0.0016, 0.0054, 0.0002, 0.0028, 0.0, 0.0014, 0.0016], [0.0705, 0.119, 0.0819, 0.0895, 0.1024, 0.0646, 0.0995, 0.068, 0.0583, 0.1028], [0.0402, 0.0371, 0.0585, 0.0475, 0.0307, 0.0267, 0.0212, 0.0221, 0.0415, 0.0446], [0.1916, 0.1409, 0.1577, 0.182, 0.1341, 0.1708, 0.175, 0.1909, 0.2248, 0.1672], [0.0, 0.0008, 0.0157, 0.0062, 0.009, 0.0036, 0.0011, 0.0008, 0.0065, 0.0], [0.1529, 0.111, 0.1083, 0.0739, 0.1188, 0.1396, 0.1118, 0.1095, 0.1219, 0.1284], [0.0621, 0.064, 0.0507, 0.0863, 0.0777, 0.0817, 0.0993, 0.0784, 0.0627, 0.0535], [0.048, 0.0559, 0.0316, 0.0172, 0.0256, 0.0477, 0.0302, 0.0114, 0.028, 0.052], [0.0002, 0.0, 0.0013, 0.006, 0.0012, 0.0001, 0.0054, 0.0106, 0.0019, 0.0012], [0.0734, 0.0558, 0.0635, 0.036, 0.0406, 0.0372, 0.0299, 0.0878, 0.043, 0.0602]]\n",
    "test_losses_280 =  [[0.027, 0.0527, 0.0316, 0.0416, 0.0524, 0.0347, 0.0225, 0.0329, 0.0243, 0.0419], [0.03, 0.0536, 0.0346, 0.0378, 0.0434, 0.0271, 0.0425, 0.0283, 0.0249, 0.0447], [0.0432, 0.0388, 0.0561, 0.0476, 0.0344, 0.0311, 0.0273, 0.0284, 0.0417, 0.0437], [0.0272, 0.0429, 0.0364, 0.0294, 0.0461, 0.0324, 0.0314, 0.0279, 0.0223, 0.034], [0.0233, 0.0286, 0.0574, 0.0418, 0.0468, 0.0364, 0.0301, 0.0289, 0.0425, 0.0236], [0.0561, 0.0363, 0.0359, 0.0242, 0.0414, 0.0489, 0.0384, 0.037, 0.0419, 0.046], [0.0348, 0.0353, 0.0432, 0.0266, 0.029, 0.0278, 0.0231, 0.0291, 0.036, 0.0416], [0.0288, 0.0261, 0.0396, 0.0556, 0.0451, 0.0296, 0.0409, 0.065, 0.0425, 0.0275], [0.043, 0.0483, 0.0362, 0.0275, 0.0372, 0.0445, 0.0278, 0.0232, 0.0343, 0.0374], [0.047, 0.0364, 0.0407, 0.0265, 0.0287, 0.0272, 0.0238, 0.0567, 0.0298, 0.0397]]\n",
    "\n",
    "# this is a dictionary with the token that was left out in each fold, with the fold as key\n",
    "loo = {0:'ML031122', 1:'MK230123', 2:'JB011222', 3:'DK011122', 4:'DC553556', 5:'WV453056', 6:'JB190123', 7:'SS291122', 8:'AS050123', 9:'TO523956', 10:'JK261022'}\n",
    "\n",
    "test_files_280 = ['NE563556', 'JM463656', 'TF483656']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results from slurm-256.out\n",
    "# 5 fold Cross-Validation\n",
    "# window_size: 8 ; stride: 4\n",
    "# batch_size: 16 ; learning_rate: 0.0001\n",
    "# Using TSST v DST Data with SELF-assessed stress\n",
    "\n",
    "train_losses_256 = [[0.0453, 0.0442, 0.0411, 0.0425, 0.0422, 0.0416, 0.0413, 0.0419, 0.0424, 0.0421], [0.0474, 0.0436, 0.0432, 0.0427, 0.0437, 0.0423, 0.0426, 0.0423, 0.0423, 0.0424], [0.0234, 0.0236, 0.0224, 0.0222, 0.0223, 0.0219, 0.0225, 0.0220, 0.0217, 0.0217], [0.0495, 0.0456, 0.0448, 0.0448, 0.0447, 0.0443, 0.0451, 0.0447, 0.0447, 0.0445], [0.0453, 0.0429, 0.0440, 0.0421, 0.0426, 0.0411, 0.0409, 0.0419, 0.0412, 0.0414]]\n",
    "validation_losses_256 = [[0.0335, 0.0325, 0.0329, 0.0319, 0.0330, 0.0326, 0.0326, 0.0378, 0.0324, 0.0319], [0.0238, 0.0396, 0.0241, 0.0288, 0.0267, 0.0242, 0.0230, 0.0196, 0.0235, 0.0211], [0.1577, 0.1465, 0.0949, 0.0985, 0.1283, 0.1395, 0.1218, 0.1369, 0.1257, 0.1100], [0.0205, 0.0246, 0.0155, 0.0176, 0.0146, 0.0201, 0.0179, 0.0266, 0.0156, 0.0214], [0.0533, 0.0338, 0.0668, 0.0349, 0.0579, 0.0299, 0.0472, 0.0424, 0.0484, 0.0516]]\n",
    "test_losses_256 = [[0.2576, 0.2412, 0.2485, 0.2184, 0.2506, 0.2427, 0.2435, 0.2957, 0.2415, 0.2187], [0.2185, 0.1633, 0.2164, 0.1962, 0.2056, 0.2159, 0.2221, 0.2460, 0.2201, 0.2336], [0.3466, 0.3265, 0.2349, 0.2419, 0.2955, 0.3149, 0.2846, 0.3110, 0.2920, 0.2634], [0.2123, 0.1923, 0.2453, 0.2287, 0.2548, 0.2127, 0.2250, 0.1842, 0.2432, 0.2051], [0.1712, 0.2145, 0.1501, 0.2105, 0.1642, 0.2237, 0.1828, 0.1930, 0.1808, 0.1746]]\n",
    "\n",
    "print(train_losses_256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results from slurm-257.out (18 files)\n",
    "# 5 fold Cross-Validation\n",
    "# window_size: 8 ; stride: 4\n",
    "# batch_size: 16 ; learning_rate: 0.0001\n",
    "# Using TSST v DST Data with PANEl stress\n",
    "\n",
    "train_losses_257 = [[0.1045, 0.0843, 0.0823, 0.081, 0.0818, 0.0844, 0.0828, 0.0845, 0.082, 0.0809], [0.0874, 0.0788, 0.0768, 0.0781, 0.0746, 0.0739, 0.0751, 0.074, 0.0742, 0.0727], [0.0873, 0.0729, 0.0668, 0.0679, 0.0676, 0.0652, 0.0673, 0.0696, 0.0683, 0.0665], [0.0802, 0.0666, 0.0663, 0.0672, 0.0686, 0.0665, 0.0668, 0.0698, 0.0654, 0.0676], [0.0886, 0.0803, 0.0782, 0.0764, 0.0765, 0.0748, 0.0769, 0.0787, 0.0749, 0.076]]\n",
    "validation_losses_257 = [[0.0504, 0.0651, 0.0516, 0.0542, 0.0376, 0.0437, 0.0416, 0.0547, 0.0416, 0.0608], [0.1006, 0.0991, 0.0948, 0.0412, 0.0405, 0.0754, 0.0698, 0.0699, 0.0833, 0.0764], [0.1414, 0.19, 0.2164, 0.1282, 0.118, 0.1032, 0.0923, 0.0886, 0.0988, 0.1175], [0.1035, 0.1162, 0.1161, 0.1352, 0.1038, 0.1088, 0.1057, 0.12, 0.1254, 0.1148], [0.0617, 0.0694, 0.0623, 0.0612, 0.0744, 0.0613, 0.0685, 0.0612, 0.0644, 0.0621]]\n",
    "test_losses_257 = [[0.0273, 0.0452, 0.0279, 0.0312, 0.0074, 0.0171, 0.0145, 0.032, 0.0146, 0.041], [0.0454, 0.0435, 0.0411, 0.0067, 0.0065, 0.0281, 0.0245, 0.0244, 0.0337, 0.0291], [0.001, 0.003, 0.0077, 0.0031, 0.0053, 0.0114, 0.0187, 0.0217, 0.014, 0.0056], [0.0155, 0.0058, 0.006, 0.0009, 0.0153, 0.0107, 0.0134, 0.0044, 0.0028, 0.0068], [0.0145, 0.003, 0.012, 0.0217, 0.0011, 0.0173, 0.0036, 0.0218, 0.0078, 0.0129]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results from slurm-259.out (30 files)\n",
    "# almost 4 (5) fold Cross-Validation\n",
    "# window_size: 8 ; stride: 4\n",
    "# batch_size: 16 ; learning_rate: 0.0001\n",
    "# Using TSST v DST Data with PANEl stress\n",
    "train_losses_259 = [[0.0518, 0.0382, 0.0395, 0.0383, 0.0378, 0.0387, 0.0383, 0.0381, 0.0383, 0.0379], [0.069, 0.0625, 0.0594, 0.0593, 0.059, 0.0586, 0.0594, 0.0583, 0.0599, 0.0588], [0.0524, 0.0459, 0.0446, 0.0464, 0.045, 0.0458, 0.046, 0.0443, 0.0433, 0.0447], [0.0651, 0.0528, 0.0525, 0.0517, 0.0521, 0.052, 0.0513, 0.0534, 0.0519]]\n",
    "validation_losses_259 = [[0.081, 0.0818, 0.1032, 0.0816, 0.1038, 0.0942, 0.0876, 0.0877, 0.08, 0.0858], [0.0182, 0.031, 0.0172, 0.0147, 0.0149, 0.016, 0.0142, 0.0142, 0.0143, 0.0142], [0.0809, 0.0611, 0.0609, 0.0694, 0.0706, 0.0589, 0.0655, 0.0679, 0.0681, 0.081], [0.0329, 0.0341, 0.0324, 0.0324, 0.0325, 0.0324, 0.0346, 0.0324, 0.0363]]\n",
    "test_losses_259 = [[0.0381, 0.0385, 0.0616, 0.0374, 0.0626, 0.051, 0.0436, 0.0434, 0.0358, 0.0434], [0.023, 0.085, 0.0242, 0.0406, 0.0428, 0.0488, 0.0371, 0.0349, 0.0318, 0.0342], [0.0207, 0.0406, 0.0406, 0.0275, 0.0267, 0.054, 0.0321, 0.0292, 0.0289, 0.0211], [0.0366, 0.0397, 0.0296, 0.031, 0.0316, 0.0308, 0.042, 0.0302, 0.0465]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot train, validation and test loss\n",
    "def plot_losses(train, val, slurm_id, token=False, test=None):\n",
    "    num_folds = len(train)\n",
    "    num_cols = 2  # Number of columns in the grid\n",
    "    num_rows = (num_folds + 1) // 2  # Calculate the number of rows needed\n",
    "\n",
    "    max_loss = max(max(max(train[i]), max(val[i])) for i in range(num_folds))\n",
    "\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(12, 6 * num_rows))\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "\n",
    "        axs[row, col].plot(range(len(train[i])), train[i], label='Train Loss')\n",
    "        axs[row, col].plot(range(len(train[i])), val[i], label='Validation Loss')\n",
    "        if token:\n",
    "            val_token = loo[i]\n",
    "            print(i, val_token)\n",
    "            axs[row, col].plot(range(len(train[i])), test[i], label='Test Loss')\n",
    "        axs[row, col].set_xlabel('Epoch')\n",
    "        axs[row, col].set_ylabel('Loss')\n",
    "        axs[row, col].set_title(\"Fold \" + str(i + 1))\n",
    "        axs[row, col].legend()\n",
    "        axs[row, col].set_ylim(0, max_loss)\n",
    "\n",
    "    # Remove any empty subplots if the number of folds is odd\n",
    "    if num_folds % 2 != 0:\n",
    "        fig.delaxes(axs[num_rows - 1, 1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"/homes/nziegenbein/losses_\" + slurm_id + \"_grid.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses_259, validation_losses_259, \"259\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses_256, validation_losses_256, \"256\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses_257, validation_losses_257, \"257\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses_280, validation_losses_280,  \"280\", token=True, test=test_losses_280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Looking closer at the split\n",
    "Here I will look closer at the training and validation data for the splits to see, whether a pattern can be established."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for random seed 19 both for test/train split and KFold for n=5\n",
    "test_files = ['/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/ML373056_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/JM463656_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/MS021222_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/ML031122_tsst_video_segment.wav']\n",
    "validation_fold_0 = ['/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/TF483656_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/WV453056_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/JK261022_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/DK011122_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/BH373056_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/TO523956_tsst_video_segment.wav']\n",
    "validation_fold_1 = ['/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/DC553556_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/AS050123_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/SB021122_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/TZ493156_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/MX463556_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/NM443056_tsst_video_segment.wav']\n",
    "validation_fold_2 = ['/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/JB011222_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/SS291122_tsst_movie_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/KO433656_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/CS181122_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/NE563556_tsst_video_segment.wav']\n",
    "validation_fold_3 = ['/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/JB190123_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/BC493156_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/KH553656_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/BU563856_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/UH473956_tsst_video_segment.wav']\n",
    "validation_fold_4 = ['/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/DK011122_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/JH373756_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/SE141122_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/BI343156_tsst_video_segment.wav', '/data/dst_tsst_22_bi_multi_nt_lab/processed/audio_files/MK230123_tsst_video_1_segment.wav']\n",
    "\n",
    "validations = [validation_fold_0, validation_fold_1, validation_fold_2, validation_fold_3, validation_fold_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = pd.read_csv(\"/data/dst_tsst_22_bi_multi_nt_lab/processed/participant.csv\")\n",
    "\n",
    "for fold, val_fold in enumerate(validations):\n",
    "    tokens = []\n",
    "    genders = []\n",
    "    panel_stress = []\n",
    "    for file in val_fold:\n",
    "        token = file.split(\"audio_files/\")[1].split(\"_tsst\")[0]\n",
    "        tokens.append(token)\n",
    "        genders.append(participants.loc[participants['token'] == token, 'gender'].values[0])\n",
    "        v1, v2 = participants.loc[participants['token'] == token, ['panel_activevas_speechstress0', 'panel_passivevas_speechstress0']].values[0]  if token in participants['token'].values else None\n",
    "        if not math.isnan(v1) and not math.isnan(v2):\n",
    "            panel_stress.append((int(v1) + int(v2)) / 2)\n",
    "        else:\n",
    "            panel_stress.append(None)\n",
    "    #print(\"tokens\", tokens)\n",
    "    male_count = sum(1 for gender in genders if gender == 'männlich')\n",
    "    female_count = sum(1 for gender in genders if gender == 'weiblich')\n",
    "    print(fold, \"male:\", male_count , \"/\", len(genders), \" female:\", female_count, \"/\", len(genders))\n",
    "    print(fold, \"mean panel_stress:\", np.mean([x for x in panel_stress if x is not None]))\n",
    "\n",
    "    train_fold = participants[~participants['token'].isin(tokens)]\n",
    "    genders = []\n",
    "    panel_stress = []\n",
    "    for token in list(train_fold[\"token\"]):\n",
    "        genders.append(participants.loc[participants['token'] == token, 'gender'].values[0])\n",
    "        v1, v2 = participants.loc[participants['token'] == token, ['panel_activevas_speechstress0', 'panel_passivevas_speechstress0']].values[0]  if token in participants['token'].values else None\n",
    "        if not math.isnan(v1) and not math.isnan(v2):\n",
    "            panel_stress.append((int(v1) + int(v2)) / 2)\n",
    "        else:\n",
    "            panel_stress.append(None)\n",
    "    male_count = sum(1 for gender in genders if gender == 'männlich')\n",
    "    female_count = sum(1 for gender in genders if gender == 'weiblich')\n",
    "    #print(fold, \"male:\", male_count , \"/\", len(genders), \" female:\", female_count, \"/\", len(genders))\n",
    "    #print(fold, \"mean panel_stress:\", np.mean([x for x in panel_stress if x is not None]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Gender seems balanced for all validation splits.\n",
    "Stress_values for fold 0 is lower than the other four splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Code to extract the losses from the out-file\n",
    "\n",
    "# Initialize lists to store the losses\n",
    "train_losses = [[] for _ in range(5)]  # Assuming 5 folds\n",
    "validation_losses = [[] for _ in range(5)]\n",
    "test_losses = [[] for _ in range(5)]\n",
    "mean_pred_epoch = []\n",
    "within_pred_tensor = False\n",
    "within_target_tensor = False\n",
    "get_pred_and_target = False\n",
    "predictions = []\n",
    "targets = []\n",
    "i=0\n",
    "\n",
    "# Read the text file and extract losses -> adapt the slurm-id\n",
    "with open('/homes/nziegenbein/slurm-290.out', 'r') as file:\n",
    "    for line in file:\n",
    "        if \"Fold\" in line and \"Epoch\" in line:\n",
    "            fold = int(re.search(r'Fold (\\d+)/(\\d+)', line).group(1)) - 1\n",
    "            train = float(re.search(r'Train Loss: (\\d+\\.\\d+)', line).group(1))\n",
    "            val = float(re.search(r'Validation Loss: (\\d+\\.\\d+)', line).group(1))\n",
    "            test = float(re.search(r'Test Loss:  (\\d+\\.\\d+)', line).group(1))\n",
    "\n",
    "            train_losses[fold].append(train)\n",
    "            validation_losses[fold].append(val)\n",
    "            test_losses[fold].append(test)\n",
    "        if \"Epoch\" in line:\n",
    "            epoch = int(re.search(r'Epoch (\\d+)', line).group(1))\n",
    "            if epoch == 9:\n",
    "                get_pred_and_target = True\n",
    "        if \"mean stress prediction\" in line:\n",
    "            print(line)\n",
    "            mean_pred_epoch.append(float(re.search(r'mean stress prediction: (\\d+\\.\\d+)', line).group(1)))\n",
    "        if \"stress_pred tensor\" in line and get_pred_and_target:\n",
    "            prediction = float(re.search(r'(-?\\d+\\.\\d+)', line).group(1))\n",
    "            predictions.append(prediction)\n",
    "            within_pred_tensor = True\n",
    "            lines_processed = 0\n",
    "        elif within_pred_tensor and lines_processed < 15:\n",
    "            #print(\"line:\", line, \"lines_processed:\", lines_processed)\n",
    "            # Extract the floats using regular expression from lines within the \"pred: tensor\" block\n",
    "            match = re.search(r'(-?\\d+\\.\\d+)', line)\n",
    "            if match is None:\n",
    "                print(\"not:\", line)\n",
    "            #print(prediction)\n",
    "            else:\n",
    "                predictions.append(float(match.group(1)))\n",
    "                lines_processed += 1\n",
    "                if lines_processed == 15:\n",
    "                    within_pred_tensor = False  # Reset the flag after extracting 15 lines\n",
    "        if \"target tensor\" in line and get_pred_and_target:\n",
    "            match = re.findall(r'(-?\\d+\\.\\d+)', line)\n",
    "            if match:\n",
    "                targets.append([float(x) for x in match])\n",
    "            within_target_tensor = True\n",
    "            target_lines_processed = 0\n",
    "        elif within_target_tensor and target_lines_processed < 3:\n",
    "            match = re.findall(r'(-?\\d+\\.\\d+)', line)\n",
    "            if match:\n",
    "                targets.append([float(x) for x in match])\n",
    "            target_lines_processed += 1\n",
    "print(\"train\", train_losses)\n",
    "print(\"val:\", validation_losses)\n",
    "print(\"test:\", test_losses)\n",
    "print(mean_pred_epoch, \"mean:\", np.mean(mean_pred_epoch))\n",
    "print(\"std\", np.std(predictions))\n",
    "targets = [item for sublist in targets for item in sublist]\n",
    "\n",
    "print(len(predictions))\n",
    "print(len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(predictions)\n",
    "#print(targets)\n",
    "\n",
    "print(np.mean(targets))\n",
    "print(np.std(targets))\n",
    "print(np.mean(predictions))\n",
    "print(np.std(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(targets, bins=100, alpha=0.5, color='orange', label=\"True Values\")\n",
    "plt.ylabel(\"True Values\", color = \"orange\")\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.set_ylabel('Predicted Values', color='blue')\n",
    "ax2.hist(predictions, bins=100, alpha = 0.5, color = 'blue', label=\"Predicted Values\")\n",
    "plt.savefig(\"/homes/nziegenbein/pred_target_distro.png\")\n",
    "plt.xlim(-1,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(targets)\n",
    "plt.xlim(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordance_correlation_coefficient(y_true, y_pred):\n",
    "    mean_true = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "\n",
    "    numerator = 2 * np.cov(y_true, y_pred)[0, 1]\n",
    "    denominator = np.var(y_true) + np.var(y_pred) + (mean_true - mean_pred)**2\n",
    "\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(concordance_correlation_coefficient(targets[0:100], predictions[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
